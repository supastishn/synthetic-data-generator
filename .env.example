# Global LoRA Configuration (applies to both distill.py and qlora.py)
TARGET_MODULES=q_proj,k_proj,v_proj

# Environment variables example for Prompt/Answer Generator
# Copy to .env and fill in values

# Required: Model for generating prompts
PROMPTGEN_MODEL=gpt-3.5-turbo

# Required: Model(s) for generating answers (comma-separated)
ANSWERGEN_MODEL=gpt-4,gpt-4-turbo

# Required for multiple models: Percentage split sum=100
MODEL_SPLIT=70,30

# Optional: Temperature 0.0-1.0
TEMPERATURE=0.7

# Required: Topics comma-separated
TOPICS=Python,JavaScript

# Required: Prompt counts per topic
AMOUNTS=3,2

# Optional: Multi-prompt generation (y/n)
MULTI_PROMPT=y

# Optional: Enable logits capture (y/n)
LOGITS=n

# Optional: Output filename
OUTPUT_FILE=conversations.json

# Optional: Batch size for prompt/answer generation (default 5)
BATCH_SIZE=7

# Optional: Enable asynchronous (simultaneous) generation (y/n, default n)
ASYNC_GEN=y

# API KEYS must be set externally through:
# - OPENAI_API_KEY
# - ANTHROPIC_API_KEY
# - etc. (based on your provider)
# See LiteLLM docs: https://litellm.vercel.app/docs

# --- DISTILLATION TRAINING (for distill.py) ---

# Required: Base model for distillation
MODEL_NAME=meta-llama/Llama-2-7b-hf

# Required: Training data file
DATA_FILE=conversations.json

# Optional: Validation data file
VALIDATION_FILE=validation.json

# Training hyperparameters
BATCH_SIZE=4
GRAD_ACC_STEPS=4
LEARNING_RATE=2e-5
ALPHA=0.7
TEMPERATURE=2.0

# Early stopping patience (epochs)
EARLY_STOPPING=3

# Log directory for TensorBoard
LOG_DIR=./logs
