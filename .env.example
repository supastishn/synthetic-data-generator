# Environment variables example for Prompt/Answer Generator
# Copy to .env and fill in values

# Required: Model for generating prompts
PROMPTGEN_MODEL=gpt-3.5-turbo

# Required: Model(s) for generating answers (comma-separated)
ANSWERGEN_MODEL=gpt-4,gpt-4-turbo

# Required for multiple models: Percentage split sum=100
MODEL_SPLIT=70,30

# Optional: Temperature 0.0-1.0
TEMPERATURE=0.7

# Required: Topics comma-separated
TOPICS=Python,JavaScript

# Required: Prompt counts per topic
AMOUNTS=3,2

# Optional: Multi-prompt generation (y/n)
MULTI_PROMPT=y

# Optional: Enable logits capture (y/n)
LOGITS=n

# Optional: Output filename
OUTPUT_FILE=conversations.json

# Optional: Batch size for prompt/answer generation
BATCH_SIZE=7

# Optional: Enable asynchronous (simultaneous) generation (y/n)
ASYNC_GEN=y

# API KEYS must be set externally through:
# - OPENAI_API_KEY
# - ANTHROPIC_API_KEY
# - etc. (based on your provider)
# See LiteLLM docs: https://litellm.vercel.app/docs
