# Global LoRA Configuration (applies to both distill.py and qlora.py)
TARGET_MODULES=q_proj,k_proj,v_proj

# Environment variables example for Prompt/Answer Generator
# Copy to .env and fill in values

# Required: Model for generating prompts
PROMPTGEN_MODEL=openrouter/deepseek/deepseek-chat-v3-0324:free

# Required: Model(s) for generating answers (comma-separated)
ANSWERGEN_MODEL=openrouter/deepseek/deepseek-r1-0528:free,openrouter/deepseek/deepseek-chat-v3-0324:free

# Required for multiple models: Percentage split sum=100
MODEL_SPLIT=75,25

# Optional: Temperature 0.0-1..0
TEMPERATURE=0.15

PROMPT_INSTRUCT="

"

# Required: Topics comma-separated
TOPICS="Python Syntax and Fundamentals,Machine Learning via PyTorch,Reinforcement Learning via Gymnasium,Game Development via PyGame,Data Analysis via Pandas,Advanced Python Topics (Concurrency, Metaprogramming)"

# Required: Prompt counts per topic
AMOUNTS=167,167,167,167,166,166

# Optional: Multi-prompt generation (y/n)
MULTI_PROMPT=y

# Optional: Enable logits capture (y/n)
LOGITS=n

# Optional: Output filename
OUTPUT_FILE=conversations.json

# Optional: Batch size for prompt/answer generation (default 5)
BATCH_SIZE=10

# Optional: Enable asynchronous (simultaneous) generation (y/n, default n)
ASYNC_GEN=y

VERBOSE_LOGGING=y # Add this line



# API KEYS must be set externally through:
# - OPENAI_API_KEY
# - ANTHROPIC_API_KEY
# - etc. (based on your provider)
# See LiteLLM docs: https://litellm.vercel.app/docs

# --- DISTILLATION TRAINING (for distill.py) ---

# Required: Base model for distillation
MODEL_NAME=meta-llama/Llama-2-7b-hf

# Required: Training data file
DATA_FILE=conversations.json

# Optional: Validation data file
VALIDATION_FILE=validation.json

# Training hyperparameters
BATCH_SIZE=4
GRAD_ACC_STEPS=4
LEARNING_RATE=2e-5
ALPHA=0.7

# Early stopping patience (epochs)
EARLY_STOPPING=3

# Log directory for TensorBoard
LOG_DIR=./logs
